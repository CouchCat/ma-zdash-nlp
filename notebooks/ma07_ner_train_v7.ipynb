{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ma_ner_train_v9.ipynb","provenance":[{"file_id":"18n3tca7KZlyBEapEL-RlL1BadAjvCEJa","timestamp":1614425939529},{"file_id":"1tckf1HO3t7jY62rxiCNq7TEdCBYruZ-S","timestamp":1612702351711}],"collapsed_sections":[],"authorship_tag":"ABX9TyN8M5rO3FDa0VPzDiq1E3Bk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"PnQWCtjUHIHQ"},"source":["# NER v9: Named Entity Recognition\n","\n","The model trained for the task of NER is capable of extracting certain keywords (entities) out of text samples. Since the data gathered for this has been retrieved mostly statistically instead of manually, the quality, therefore, is highly tied to the algorithms used. In the case for the application using this model, certain entities have been collected:\n","\n","* PROD: for certain products\n","* BRND: for brands\n","* PERS: people names\n","\n","The following tags are simply in place to help better categorize the previous tags:\n","\n","\n","* MATR: relating to materials, e.g. cloth, leather, seam, etc.\n","* TIME: time related entities\n","* MISC: any other entity that might skew the results\n","* O: For tokens that do not belong to the desired entities\n","\n","The IOB-notation is used here in order to retrive n-grams, such as \"Tommy Hilfger\", which is tagged by `B-BRND` and `I-BRND` for both words respectively in order to group them together into a single entity `BRND`. Performance for the `I-BRND` is rather low though (see evaluations) since not many training example could be gathered for this in this version.\n","\n","## Notes:\n","* Use Cased version of BERT since NER works best this way\n","* An additional `PAD` entity has been included for training purposes in order to distinguish tokens from paddings created by the fixed max sequence length\n","* Results: Naturally, the `O` entity has the highest quantity, making it the most predicted tag. More important are is the performance for the `PROD` and `BRND` entities since these are considered most insightful for the application for now. `PERS` entity, while useful for extracting customer names, this won't be really visible in the final version for privacy reasons."]},{"cell_type":"code","metadata":{"id":"JAkNe802Yc1D","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614726258448,"user_tz":-60,"elapsed":21637,"user":{"displayName":"Carlos Tan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0VjHaxgzs8uvbhG9roTvit9LTXRto76Vdwkxh4w=s64","userId":"01125919987247037641"}},"outputId":"39ee3ebe-b9a9-46f4-db24-54ef248fe108"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1XrFSaG2HcUk"},"source":["!pip install transformers==3.0.2 >> /dev/null\n","!pip install scikit-plot >> /dev/null"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LvB211MgHhGD"},"source":["import pandas as pd\n","import numpy as np\n","import torch\n","from tqdm import tqdm, trange\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, Dataset, TensorDataset\n","from transformers import BertTokenizer, BertForTokenClassification, DistilBertTokenizer, DistilBertForTokenClassification, AdamW, get_linear_schedule_with_warmup\n","from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n","from sklearn.model_selection import train_test_split, StratifiedKFold\n","import matplotlib.pyplot as plt\n","import scikitplot as skplt\n","import seaborn as sns\n","sns.set(style='white', context='notebook', palette='deep')\n","import time\n","import re\n","import gc\n","\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uHEleVHYHigb","executionInfo":{"status":"ok","timestamp":1614726272022,"user_tz":-60,"elapsed":35194,"user":{"displayName":"Carlos Tan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0VjHaxgzs8uvbhG9roTvit9LTXRto76Vdwkxh4w=s64","userId":"01125919987247037641"}},"outputId":"3b469849-c373-42db-c993-cab989474ded"},"source":["if torch.cuda.is_available():    \n","    device = torch.device(\"cuda\")\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla T4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_qlP9Py3Hk7g"},"source":["PROJECT_PATH = '/content/drive/MyDrive/Colab/data/ma_data/'\n","DATA_RAW = PROJECT_PATH + 'ner_train_feedback04_glove_twitter.csv'\n","DATA_CONCAT = PROJECT_PATH + 'ner_train_concat.csv'\n","\n","MODEL_PATH = PROJECT_PATH + 'ner09_model'\n","RESULTS_PATH = PROJECT_PATH + 'ner_results_v9'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lc4RAJb3IVLx"},"source":["# Data\n","\n","* Sentence idx is used for grouping the individual tokens (words) together since the current format assigns each token to a single tag. \n","\n","* Data is natually unbalanced since there will always be more `O` tags\n","\n","* Would be nice to have more `I-...` tags since these are more interesing to gather and train for though since most target words are of single nature, a good performance for `B-...` tags should suffice for the prototype for now"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":363},"id":"umce2Jg1HomY","executionInfo":{"status":"ok","timestamp":1614726274793,"user_tz":-60,"elapsed":2755,"user":{"displayName":"Carlos Tan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0VjHaxgzs8uvbhG9roTvit9LTXRto76Vdwkxh4w=s64","userId":"01125919987247037641"}},"outputId":"2cd2cfb1-3367-44d7-fdff-5ccb4ed7193b"},"source":["df = pd.read_csv(DATA_RAW)\n","df[60:70]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentence_idx</th>\n","      <th>word</th>\n","      <th>tag</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>60</th>\n","      <td>0</td>\n","      <td>step</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>61</th>\n","      <td>0</td>\n","      <td>very</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>62</th>\n","      <td>0</td>\n","      <td>loud</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>63</th>\n","      <td>0</td>\n","      <td>,</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>64</th>\n","      <td>0</td>\n","      <td>which</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>65</th>\n","      <td>0</td>\n","      <td>is</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>66</th>\n","      <td>0</td>\n","      <td>not</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>67</th>\n","      <td>0</td>\n","      <td>only</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>68</th>\n","      <td>0</td>\n","      <td>noticeable</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>69</th>\n","      <td>0</td>\n","      <td>to</td>\n","      <td>O</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    sentence_idx        word tag\n","60             0        step   O\n","61             0        very   O\n","62             0        loud   O\n","63             0           ,   O\n","64             0       which   O\n","65             0          is   O\n","66             0         not   O\n","67             0        only   O\n","68             0  noticeable   O\n","69             0          to   O"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ldib_dcsJ4KW","executionInfo":{"status":"ok","timestamp":1614451447378,"user_tz":-60,"elapsed":7091,"user":{"displayName":"Carlos Tan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0VjHaxgzs8uvbhG9roTvit9LTXRto76Vdwkxh4w=s64","userId":"01125919987247037641"}},"outputId":"215c50bd-f9f7-4057-c268-036729d23341"},"source":["df[\"tag\"].value_counts()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["O         335012\n","B-PROD     10477\n","B-TIME      5094\n","B-PERS      2624\n","B-BRND      2248\n","B-MATR      1665\n","I-PERS       520\n","B-MISC       475\n","I-BRND       150\n","Name: tag, dtype: int64"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZY6hU3GBkcOA","executionInfo":{"status":"ok","timestamp":1614451447380,"user_tz":-60,"elapsed":6818,"user":{"displayName":"Carlos Tan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0VjHaxgzs8uvbhG9roTvit9LTXRto76Vdwkxh4w=s64","userId":"01125919987247037641"}},"outputId":"9f49c3cc-13f8-4cc0-aaa5-6b09e66b698c"},"source":["len(df)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["358265"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"EvDhBW-OIg6r"},"source":["# Pre-processing\n","\n","* An additional `PAD` entity has been included for training purposes in order to distinguish tokens from paddings created by the fixed max sequence length"]},{"cell_type":"code","metadata":{"id":"93VdynZ7Qd2s"},"source":["def get_texts_and_labels(df, label2idx):\n","    agg_func = lambda data: [(w, t) for w, t in zip(data[\"word\"].astype(str).tolist(), data[\"tag\"].astype(str).tolist())]\n","    grouped_data = df.groupby(\"sentence_idx\").apply(agg_func)\n","\n","    texts = [[pair[0] for pair in gd] for gd in grouped_data]\n","    label_tags = [[pair[1] for pair in gd] for gd in grouped_data]\n","    labels = [[label2idx.get(l) for l in label] for label in label_tags]\n","\n","    return texts, labels"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VyOE027aWDvI","executionInfo":{"status":"ok","timestamp":1614726292076,"user_tz":-60,"elapsed":1990,"user":{"displayName":"Carlos Tan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0VjHaxgzs8uvbhG9roTvit9LTXRto76Vdwkxh4w=s64","userId":"01125919987247037641"}},"outputId":"a27350ae-6d2e-414f-f48a-e3d292ab7e6e"},"source":["# Adding PAD here since Bert Tokenizer adds this token as padding\n","unique_labels = list(set(df[\"tag\"].values))\n","unique_labels = [label for label in unique_labels if str(label) != 'nan']\n","unique_labels.append(\"PAD\")\n","unique_labels_sorted = sorted(unique_labels)\n","\n","num_labels = len(unique_labels_sorted)\n","label2idx = dict(zip(unique_labels_sorted, range(num_labels)))\n","texts_raw, labels_raw = get_texts_and_labels(df, label2idx)\n","\n","label2idx"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'B-BRND': 0,\n"," 'B-MATR': 1,\n"," 'B-MISC': 2,\n"," 'B-PERS': 3,\n"," 'B-PROD': 4,\n"," 'B-TIME': 5,\n"," 'I-BRND': 6,\n"," 'I-PERS': 7,\n"," 'O': 8,\n"," 'PAD': 9}"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"id":"RVsQ82pXtje6","executionInfo":{"status":"ok","timestamp":1614726292775,"user_tz":-60,"elapsed":1155,"user":{"displayName":"Carlos Tan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0VjHaxgzs8uvbhG9roTvit9LTXRto76Vdwkxh4w=s64","userId":"01125919987247037641"}},"outputId":"a378392f-ca18-4df2-f8d0-6353b3da7b8e"},"source":["fig, ax = plt.subplots(1,figsize=(14,5))\n","\n","sns.histplot([len(x) for x in texts_raw], ax=ax, bins=250)\n","ax.set_title('The number of words in each data', fontsize=14)\n","ax.set_xlim(0,300)\n","ax.set_xlabel('number of words')\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAA1UAAAFTCAYAAADROSbyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZxcdZnv8U8nYItJJAHFgBJwgUcFriyCKOAy4zh6lYkLLgwoCI6ijorL6IgouCDoMDOKyICAAwKig47gdvWOOiy5oKMoaEQfQA2rgJBowpKAJPePcxqqi6ruqj7VtX7er1de3XXOqVNPVR2K+vbvd54ztmHDBiRJkiRJMzOn1wVIkiRJ0iAzVEmSJElSBYYqSZIkSarAUCVJkiRJFRiqJEmSJKkCQ5UkSZIkVWCokqQ2RcS2EbEhIp7e61o6KSLOiIhv9rqOWhGxNCKuiYg/R8QZva6nVkTsFxEzvi5JRBwdEcs7WdNsiIiDI+LODu3rzog4uBP7kqR+slGvC5CkftLCl+QzgaO7UIoKpwOnAZ8BOvLFvo8cT/G81EREHA3sl5k79roWSZqKoUqSJtuy5veXAKfWLbsHWNTVigZcRGycmffN4H4Lgc2B72bmTZ2vrOU6HpaZ93Z6v5l5J8MXFCVpJBmqJKlGZt4y8XtE/LF+Wbl8IlRtExHHAnsBK4B3ZOZ/1Wz3VOCfgGdThLHvA++s31/N9tsCvwP2Aw5rtN+IeC7w38CjM/P2uvvtnpk/qdnmfwMfA54K/ATYH3gicEL580LgoMy8o66OI4G3AfOA84C3ZOY95box4B+ANwFbAdcCn8jMs+tq+Vvg74Bnltuf2OD5LgI+BfwN8HDg/5XP9Zc1zwHgBxEB8LzMvLBuH4cBh2fmk8vbzwf+C3h/Zh5XLjsbWJuZbyhvvxz4MLA9cBtwMvDxzNxQrl8BnAEsAV5e7u+VEfE64KPAo4EfAP+nrpaty+e5T/l8rgeOzswv1T/3cvujqRmFKac3Pqp8vPcCjwDOB96amXc32kd5vymPs4jYHTgG2BV4GPBz4B8y87KafWwKHAe8lOKPBr8ra/9yzTZ/CXwaeDzwP8Ahmfm7Kep6EsUo457AdcC7G2xzHPAyitf6VuA/gA9l5tpymuBR5XYTI8ivz8wzIuJdwMEUx/EfKd6L92TmH5vVI0mzyXOqJGnmjqEIKE8Dfgx8KSLmA0TElsDFwHJgD+D5wHzggoiY7rO36X7b9GHgcOAZFF+Uvwx8CHgj8FxgBx46lfE55eP+JfAK4AXAJ2rWfww4FHgrRVg7FjglIl5ct59jgZPKbc5vUt8ZZW1LKV6ju4HvRMQmwKVlfZR1bFkuq3chEBGxuLz9XOD28mftc7qQYsPdKILifwI7Af8IvB/4+7r9vgv4NfB04IiIeEZZ7+eAnYFvAB+pu89JFEHoeWXth1N84W/HPsCOFMfLqykCxzuabdzicbYAOKvc9x7AFcC3I2Lzch9jwLcpXqfXU7xn7wJqR+fGKV6nQyiC8kKKMNqsrjnA1yi+ZzyzvN/R5X5q3VWuewrwFuA1wAfKdV8G/hlIivd/y3IZwHqK13cHigC/B06llNRDjlRJ0sz9a2Z+AyAijgBeR/GFexnwZuDKzHzfxMblSMdKii/q/zPD/bbjg5l5Sbmfkym+dO6WmT8tl51JMSpW636K0YA7geUR8T7g9Ih4f7n+XcALJvYL/C4i9qAIWd+q2c9nMvMrzQqLiO0oRqiek5kXl8teSzG6c0BmnhYRt5Wbr2w2upeZv46IWyiCzLkUYep44IMRsRGwLfA4ylBV1n9RZh5V3r66rOV9TP5SflFmfrKm3i8C38/MY2rutztFwJywDfDVzLxy4rVp9vynsBo4LDPvB34VEedRBNxjm2w/7XGWmT+ovUNEvI0iqL4IOJsiiD0T2CEzf1Vu9tu6x9mIYsQsy30cD3w+IsYmRvjqPJ8inD0+M68v73M4cEntRpn50ZqbKyLi48B7KI7de8oGGX+uf/8z81N193svRZA8KDPXN6hHkmaVoUqSZu7nNb/fXP7covy5G/DsJl3TnsjUoWqq/c60vlvLn7+oW1a/35+XgWrCZRRTxp5IMcrwcIrRpNov0htTTFOs9ZNpansKxWjDA1PQMvNPEfELii/j7bgIeG5EXADsThEY3lz+vgPwm8y8seZxv1V3/2XAURHxyMxc3aT+p1CMTtW6jMmh6tPAyRHxQoopeF/LzMvbfC5XlYFqws0Uo3nNTHucRcQWFNMWnwc8BpgLbEIx5Q5gF+D3NYGqkXUTgaqmrodRjICubLD9U4CbJgJV6UcU7/kDImI/ihGnJ1GMsM0t/00pIv6CYuTsKcCm5X0eBizmwf9mJKlrnP4nSTP3QPOFmr/Wz6n5+S2KEabaf9sB07Utn2q/E19Kx2q233i6/QAbyv3VL2vn/wMT2+7L5Oe0A8U0wVp3tbHfeu22Kb+QIjA8C7g2M2+tWfZcHhylaudx264/M0+nON/o3ynO17q0PG+qHfUNPaZ7j1o5zs6kCJjvpHiNdgZupAghrfpzg7omHn9GImJP4EvAdymOqV2AI2l+PE/cbxuK5/wr4JUUwfKQcnU7z0mSOsaRKkmaHT8FXgVcN5POd1P4Q/lzy5rfd+7g/neKiHmZOREq9qQ4t+Y3FF+g1wHb1E8pm4Ff8eD5NhPT/x5JcZ7Tv7e5rwuBfwMO4MEAdWF5+8kUIxq1j7tX3f33Bm7MzDXT1Ltn3bL625QjYp8DPldOnXwHs9uCv5XjbG/g7Zn5LYCIeAyTO1r+DNgyIp4yzWhVO34FPDYits7MG8plezA5hO1FMZr1wBTAMjDVupeHjlw9nSI8vXNiVC8iXtKhuiVpRgxVkjQ7PkvR/e7LEfEJigD0BIovwO+e5gv8VK4FbgCOjoh/pDhn6Mjq5T5gI4pzZT5C0d3vOODUiZBVnktzfNnc4GKKKVt7Ausz83OtPkhmXlNO1zslIt5I0dDhGIpzir7YTsE151UdSNHhEIpQdWr5fC6s2fyfgR+XI0hfpBjBeTdwxDQPcwLFyNP7ga9QjIC9rHaDiPg0RRe6q4FHAi8ErmrnucxAK8fZ1cCBEfEjio6On2RyE4rvU0zN+2pEvLPc/knAvMxs1mRkOt+jaPTxhXKfmwD/yuQRr6spgtcBFFMp/5oH378JKyi6bO5Kcb7dGuAainB2eET8J8Xxd/gM65SkjnD6nyTNgsy8meIv8euB7wC/pPgCvK78N9P93kfRIe0JwJUUHf6mCwTtuIii1v+m6N72A4r23hM+SDHy8p5yu/+iOIdpJk0ZXk9xbtnXy5+PAF440b59BnXPLX+SmSuAm5h8PhVlk45XljUvpwiNx9Gg5XutzPwhxflTb6Y4V+3lPHQEag5Fs4urKF6XW4GDZvBcWtbicXYIRfi9nGK63eepOQeubOzwIoqW9mdTjDJ9mgpT6cp9voziNfkR8AWKzpHrarb5BkUr+E9RvKZ/RdGdstZXKToTfp8iMO6fmT+nGAF8F8Vr/QaK41GSemZsw4Z2p65LkiRJkiY4UiVJkiRJFRiqJEmSJKkCQ5UkSZIkVWCokiRJkqQKhralekSMU7TK/T1w/zSbS5IkSRo9cymu3ffjzJxxd96hDVUUgeqSXhchSZIkqe/tAyyb6Z2HOVT9HuCcc85h8eLFva5FkiRJUp+55ZZbOOCAA6DMDjM1zKHqfoDFixfzuMc9rte1SJIkSepflU4XslGFJEmSJFVgqJIkSZKkCgxVkiRJklSBoUqSJEmSKjBUSZIkSVIFhipJkiRJqsBQJUmSJEkVGKokSZIkqQJDlSRJkiRVsFGvCxhWp1+wnFVr1k1atmjBOIcu3bFHFUmSJEmaDYaqWbJqzTpWrl7b6zIkSZIkzTKn/0mSJElSBYYqSZIkSarA6X+aMc8bkyRJkgxVqsDzxiRJkiSn/0mSJElSJYYqSZIkSarAUCVJkiRJFRiqJEmSJKkCQ5UkSZIkVWCokiRJkqQKDFWSJEmSVIGhSpIkSZIqMFRJkiRJUgWGKkmSJEmqYKNeF6DOOP2C5axas27SskULxjl06Y49qkiSJEkaDYaqIbFqzTpWrl7b6zIkSZKkkeP0P0mSJEmqwFAlSZIkSRUYqiRJkiSpAs+p0sCwGYckSZL6kaFKA8NmHJIkSepHTv+TJEmSpAoMVZIkSZJUgaFKkiRJkiowVEmSJElSBYYqSZIkSarAUCVJkiRJFRiqJEmSJKkCQ5UkSZIkVWCokiRJkqQKDFWSJEmSVMFG3XiQiNgcOAt4InAvcA3wpsz8Q0TsCZwCbAKsAA7MzNvK+zVdJ0mSJEn9oFsjVRuAT2ZmZOZOwG+A4yJiDnA28NbM3B64GDgOYKp1kiRJktQvuhKqMnNlZl5Ys+iHwDbAbsDazFxWLj8ZeFX5+1TrJEmSJKkvdP2cqnIE6s3A14ElwHUT6zLzdmBORGw2zTpJkiRJ6gu9aFTxGeBO4MQePLYkSZIkdVRXQ1VEHA9sB7w6M9cD11NMA5xY/yhgfWaunGadJEmSJPWFroWqiPg4xXlSL83MdeXiy4FNImLv8vZhwHktrJMkSZKkvtCtluo7AO8HrgYujQiA32XmyyLitcApEfFwyrbpAJm5vtk6SZIkSeoXXQlVmflLYKzJukuBndpdJ0mSJEn9oBeNKiRJkiRpaBiqJEmSJKkCQ5UkSZIkVWCokiRJkqQKDFWSJEmSVIGhSpIkSZIqMFRJkiRJUgWGKkmSJEmqwFAlSZIkSRUYqiRJkiSpAkOVJEmSJFVgqJIkSZKkCgxVkiRJklSBoUqSJEmSKjBUSZIkSVIFhipJkiRJqsBQJUmSJEkVGKokSZIkqQJDlSRJkiRVYKiSJEmSpAoMVZIkSZJUgaFKkiRJkiowVEmSJElSBYYqSZIkSarAUCVJkiRJFRiqJEmSJKkCQ5UkSZIkVbBRrwsYdKdfsJxVa9ZNWrZk8YIeVSNJkiSp2wxVFa1as46Vq9dOWrZw/niPqpEkSZLUbU7/kyRJkqQKDFWSJEmSVIHT/6QZqD+XbtGCcQ5dumMPK5IkSVKvGKqkGWh0Lp0kSZJGk9P/JEmSJKkCQ5UkSZIkVWCokiRJkqQKDFWSJEmSVIGNKtSX6rvrLVm8oIfVSJIkSc0ZqtSX6rvrLZw/3sNqJEmSpOac/idJkiRJFRiqJEmSJKkCQ5UkSZIkVWCokiRJkqQKDFWSJEmSVIHd/1pU3+IbbPOt7ml0/C1aMM6hS3fsUUWSJEmaYKhqUX2Lb+hcm2+/MGs6jY4/SZIk9QdDVR/wC7MkSZI0uLoWqiLieOAVwLbATpm5vFy+Alhb/gN4X2Z+t1y3J3AKsAmwAjgwM2/rVs3DzNExSZIkqTO6OVJ1PvBp4JIG6/abCFkTImIOcDZwcGYui4gjgeOAQ2a90hHg6JgkSZLUGV3r/peZyzLzhjbushuwNjOXlbdPBl7V+co0yMbGel2BJEmSRl2/nFN1TkSMAcuAIzLzj8AS4LqJDTLz9oiYExGbZebKXhWq/rLpvHGnMkqSJKmn+iFU7ZOZN0TEOPAp4ETgwB7XpA6bzeDjVEZJkiT1Us9D1cSUwMxcFxEnAV8vV10PbDOxXUQ8Clg/yKNUozxVzeAjSZKkYdXTUBUR84CNMvNP5fS/1wBXlKsvBzaJiL3L86oOA87rRl31oyqdushvo6lqM9n3bNUnSZIkqX3dbKl+AvByYDHwvYi4A9gX+GpEzAXmAlcBbwHIzPUR8VrglIh4OGVL9W7UWj+q0qmL/HZq37NZnyRJkqT2dC1UZebbgbc3WLXLFPe5FNhp1ooacqM83VCSJEnqlp6fU6XZ06wzntMFJUmSpM4xVA25Rg0inC4oSZIkdU7XLv4rSZIkScPIUCVJkiRJFRiq9AAbW0iSJEnt85wqPaBRY4tFC8Y5dOmOPaxKkiRJ6m+Gqj7Vq1GjRo0tJEmSJDVnqOpTg9oO3SmEkiRJGjWGqj42iO3QG4XBfg+CkiRJUhWGKnVcfRjs9yAoSZIkVdFy97+IeGWT5ft1rhxJkiRJGiztjFSdDpzXYPnngK90ppzuGsRzloaJ519JkiRpGEwbqiLiCeWvcyLi8UDtV+EnAAPbKm4Qz1kaJoPQjKPf65MkSVLvtTJSdS2wgSJM/aZu3S3A0R2uSX1ktkeT+j3Y9nt9kiRJ6r1pQ1VmzgGIiIsy8zmzX5L6ySCMJkmSJEm91PI5VQaq0eVozfQ8P0ySJGl0tRyqyvOpjgF2BubXrsvMJR2uSxoozUb0Fi0Y59ClO/aoKkmSJHVDO93/vkhxTtW7gbtnpxxpcDUa0ZMkSdLwaydU7QDslZnrZ6sYSZIkSRo0LV/8F7gY2GW2CpE6yXOcJEmS1C3tjFStAL4TEV+jaKX+gMz8UCeLkqqaqmvhmrvunbR8Ns97MtxJkiQNv3ZC1Tzgm8DGwNazU47UOc26Fv7xzu6d+2QDC0mSpOHXTkv1189mIdKwsoGFJEnScGunpfoTmq3LzN92phxJkiRJGiztTP+7FtgA1J4lsqH8ObdjFUmSJEnSAGln+t+kToERsRg4Crik00VJkiRJ0qBop6X6JJl5C3A4cGznypEkSZKkwdLO9L9GAnhEJwqR1F/quxbasVCSJKmxdhpVXMKD51BBEaZ2AD7S6aIk9Z5dCyVJklrTzkjVaXW37wKuzMxrOliPJEmSJA2UdhpVnDmbhUiSJEnSIGpn+t/GwJHAa4GtgJuBs4BjMvPe2SlPkiRJkvpbO9P/PgnsARwGXAdsA3wQeCTwzs6XJnXH2Nj020iSJEnNtBOqXgk8LTPvKG9nRPwUuBJDlQbYpvPGH9LpDmDJ4gU9qkiSJEmDpJ1Q1ezv+f6dXwOvUae7hfPHe1SNJEmSBkk7oeo84BsR8WHgeorpf0eWyyVJkiRpJLUTqt5LEaI+S9Go4ibgXOBjs1CXJEmSJA2EaUNVROwF/E1mvg/4UPlvYt0ngF2BH85ahZIkSZLUx+a0sM0RwMVN1v038IHOlSNJkiRJg6WVULUz8J0m674H7Na5ciRJkiRpsLQSqh4JPKzJuo0B+05LkiRJGlmthKpfAy9osu4F5XpJkiRJGkmtdP/7V+CUiJgLnJ+Z6yNiDvBSik6A75rNAiVJkiSpn00bqjLzixGxGDgTGI+I24FHAeuAozLz3FmuURo6Y14yW5IkaWi0dJ2qzPyXiDgNeCawOXAHcFlmrp7N4qRhtem8cU6/YDmr1qybtHzRgnEOXbpjj6qSJEnSTLR88d8yQH13FmuRRsqqNetYuXrtpGWOYEmSJA2elkNVFRFxPPAKYFtgp8xcXi7fnmJa4cTo1+sy85rp1knDqtEI1pLFNtiUJEnqZ610/+uE84FnA9fVLT8Z+Gxmbk/R9OKUFtdJQ2tiBGvi3+o77+11SZIkSZpCV0JVZi7LzBtql0XEFsCuwESji3OBXSPi0VOt60a9kiRJktSqbo1UNbI1cFNm3g9Q/ry5XD7VOkmSJEnqG70MVZIkSZI08HoZqm4AHlteVJjy51bl8qnWSZIkSVLf6FmoyszbgCuA/ctF+wM/y8w/TLWu+5VKkiRJUnNdCVURcUJE3Ag8DvheRPyyXHUY8LaIuBp4W3mbFtZJkiRJUl/oynWqMvPtwNsbLP818Iwm92m6TpIkSZL6RVdClSRNp/6ixwCLFoxz6NIde1SRJElSawxV0ohrFGaWLF7Q9TomLnosSZI0aAxV0ohrFGYWzh/vUTWSJEmDx+tUSZIkSVIFhipJkiRJqsBQJUmSJEkVGKqkATU21usKJEmSBDaqkAbWpvPGbUMuSZLUBwxV0gCzDbkkSVLvOf1PkiRJkiowVEmSJElSBU7/k0ZEo/Ovlixe0KNqJEmShoehShoRjc6/Wjh/vEfVSJIkDQ+n/0mSJElSBYYqSZIkSarAUCVJkiRJFXhOlaRZ48WJJUnSKDBUSZo1XpxYkiSNAqf/SZIkSVIFhipJkiRJqsBQJUmSJEkVGKokSZIkqQJDlSRJkiRVMBLd/xq1dV6yeEGPqpFm19hYryuQJEkaLSMRqhq1dV44f7xH1Uiza9N54w/5Q8Js/xFhtv5wYUCUJEmDYCRClTRq6v+QMNt/RJitP1w0CojgBYQlSVJ/MVRJ6mteQFiSJPU7G1VIkiRJUgWGKkmSJEmqwFAlSZIkSRUYqiRJkiSpAkOVJEmSJFVgqJIkSZKkCgxVkiRJklSBoUpSS8bGel2BJElSf/Liv5Jasum8cU6/YDmr1qybtHzJ4gU9qkiSJKk/GKoktWzVmnWsXL120rKF88d7VI0kSVJ/cPqfJEmSJFVgqJIkSZKkCgxVkiRJklSBoUqSJEmSKjBUSZIkSVIFhipJkiRJqsBQJUmSJEkVeJ0qSQNnbKzx8kYXJ160YJxDl+7YhaokSdKoMlRJGjibzht/SIBasnhBw4sTT8UQJkmSOsFQJWkg1QeohfPHK+9DkiRpJvoiVEXECmBt+Q/gfZn53YjYEzgF2ARYARyYmbf1okZJkiRJaqQvQlVpv8xcPnEjIuYAZwMHZ+ayiDgSOA44pFcFSpIkSVK9fu7+txuwNjOXlbdPBl7Vw3okSZIk6SH6KVSdExE/j4iTImIhsAS4bmJlZt4OzImIzXpWoSRJkiTV6ZdQtU9mPg3YHRgDTuxxPZIkSZLUkr4IVZl5Q/lzHXASsBdwPbDNxDYR8ShgfWau7EmRkiRJktRAz0NVRMyLiE3L38eA1wBXAJcDm0TE3uWmhwHn9aZKSZIkSWqsH7r/PQb4akTMBeYCVwFvycz1EfFa4JSIeDhlS/XelSlJkiRJD9XzUJWZvwV2abLuUmCn7lYkSZIkSa3reaiSpFF3+gXLWbVm3aRlixaMc+jSHXtUkSRJaoehSlJXjY31uoL+s2rNOlauXtvrMiRJ0gwZqiR11abzxh8yMrNk8YIeVjQ76p+jI0+SJA0vQ5WkrqsfmVk4f7yH1cwOR58kSRodPW+pLkmSJEmDzFAlSZIkSRU4/U/SUOtUYww79EmSpGYMVZKGWqPGGNB+cwzPkZIkSc0YqiQNvUaBqFlzDFu+S5KkdhmqJKlGOy3fDWCSJAkMVZL0EK22fO/U1EJJkjTYDFWSVEE7UwslSdJwsqW6JEmSJFVgqJIkSZKkCgxVkiRJklSB51RJ0hDw4sSSJPWOoUqShkC7FyeuD2EGMEmSZs5QJUkjqN0QJkmSmvOcKknqQ15YWJKkweFIlST1oWYXFnaaniRJ/cdQJUl9apim6BkQJUnDzFAlSQOknWmB/TSFcJgCoiRJ9QxVkjRAGk0LXLJ4QcvbTrW9JEmaGUOVJA2Y+lGfhfPHW952uu0lSVL77P4nSZIkSRUYqiRJkiSpAqf/SZKm1Oy8rDV33Ttpud38JEmjylAlSV3QT5342tXsvKw/3jl5+SA/R0mSqjBUSVIXjEInvlF4jpIkNWKokqQuGYVOfP30HOsDntMTJUmzxVAlSRpKXnBYktQtdv+TJEmSpAocqZIk9R2n7kmSBomhSpLUd5y6J0kaJE7/kyRJkqQKDFWSpJ5o57pWXgNLktTPnP4nSeqJdq5r1WzbQTjXalDrliS1zlAlSeqZdq5rNajnWQ1q3ZKk1jn9T5I0EtPrRuE5SpJ6w5EqSVJbU/EGVSenENbvZ8niBay5696hfv0kSc0ZqiRJQHtT8QZVo+c41QhWs6BZv5+F88f5453D//r1E69lJqmfGKokSSNtqlG6QQ2aoxA4PFdNUj8xVEmSRt6ghqdmuh047HAoadQZqiRJA2tQm08Mat3NOGokadQZqiRJA2tQG2w0qnuzBeMcMgIjO+2Oag3i+ytp9BiqJEkDbVCn7jVqdjFM0+iajca1O6o1qO+vpNFiqJIkqU90qjthvZnso75FfLtt4xuNxjXbdtimQ8JoNAuR9KC+D1URsT1wJrA5cAfwusy8prdVSZLUHZ3oTjiTfdS3iJ9J2/hGo3HtPsfZNJvBx/PMWmP41LDo+1AFnAx8NjPPjogDgVOAv+hxTZIkdU0npsD1+zS6qvXNZLSr/jE7MaI3sbwT9XVCv08pbec9kPpZX4eqiNgC2BX4q3LRucCJEfHozPzDNHefC3DLLbcw577VbLz+3kkr71+3njn33dfS8na27cW+R6G+UXiO/V7fKDxH6+v9PqxvuOubzec4f6P1nHbeMtbcfd+kbbdYtAn3rPtzw+Vz7ru75X38aeXdrLlrch2rNr6bu+6+r+HyVutb8IiNeeEzt6WR71y2YtL2zZ7LVPv408rbHlLfnPsexo033jjt43Wjvjn3/anjr9N027ej1ec4k+euyRq9j41e106/prfccsvEr3Or7Gdsw4YN1auZJRGxG/CFzNyhZtlVwIGZ+dNp7rs3cMkslyhJkiRp8O2Tmctmeue+Hqmq6MfAPsDvgft7XIskSZKk/jMX2JIiO8xYv49UbQFcDWyemfdHxFyKZhXbtTD9T5IkSZJm3ZxeFzCVzLwNuALYv1y0P/AzA5UkSZKkftHXI1UAEfFkipbqi4BVFC3Vs7dVSZIkSVKh70OVJEmSJPWzvp7+J0mSJEn9zlAlSZIkSRUYqiRJkiSpAkOVJEmSJFUwlBf/jYjtKToGbk5xXavXZeY1va1K3RARK4C15T+A92XmdyNiT+AUYBNgBXBg2bJfQyAijgdeAWwL7JSZy8vlTT8L/JwYPlMcByto8LlQrvOzYchExObAWcATgXuBa4A3ZeYfpnq/PRaGyzTHwQbgF8D6cvPXZuYvyvvtC/wTxXfky4HXZ+bd3a5fnRMR5wOPp3i/7wTelplXdPo7wrCOVJ0MfDYztwc+S/EhqdGxX2buXP77bkTMAc4G3loeExcDx/W2RHXY+cCzgevqlk/1WeDnxPBpdhxA3ecCgJ8NQ2sD8MnMjMzcCfgNcNxU77fHwlBqeBzUrH9WzWfCRKCaD5wK7JuZTwLWAO/pduHquIMy82mZuQtwPPD5cnlHvyMMXaiKiC2AXYFzy0XnArtGxKN7V5V6bDdgbWYuK2+fDLyqh/WowzJzWWbeULtsqs8CPyeGU6PjYBp+NgyhzFyZmRfWLPohsIXHk7YAAAdSSURBVA1Tv98eC0NmiuNgKi8CflIzInEy8OpZKE9dlJl/qrm5KbB+Nr4jDF2oArYGbsrM+wHKnzeXyzUazomIn0fESRGxEFhCzV+uM/N2YE5EbNazCtUNU30W+Dkxeuo/F8DPhqFXjkC9Gfg6U7/fHgtDrO44mHBhRFwREcdGxHi5bNJxAFyP/18YChFxWkRcDxwDHMQsfEcYxlCl0bZPZj4N2B0YA07scT2Ses/PhdH1GYpzKHzPR1v9cbAkM59OMV34qcAHe1WYuiMz35CZS4AjKM6Z67hhDFU3AI+NiLkA5c+tyuUachNTfzJzHXASsBfFX5oeGPKPiEcB6zNzZU+KVLdM9Vng58QIafK5AH42DLWyccl2wKszcz1Tv98eC0OqwXFQ+5mwGjiNJp8JFCNX/n9hiGTmWcDzgBvp8HeEoQtVZaeeK4D9y0X7Az/LzD/0rip1Q0TMi4hNy9/HgNdQHAuXA5tExN7lpocB5/WmSnXLVJ8Ffk6Mjik+F8DPhqEVER+nOE/qpWWYhqnfb4+FIdToOIiIRRGxSfn7RsB+PPiZ8B1g94jYrrx9GPAf3a1anRQR8yNi65rb+wIrgY5/RxjbsGFDp+vvuYh4MkUbxEXAKoo2iNnbqjTbIuIJwFeBueW/q4C3Z+bvI+JZFJ1bHs6DrXJv7VWt6qyIOAF4ObAYuB24IzN3mOqzwM+J4dPoOAD2pcnnQnkfPxuGTETsACwHrgbuKRf/LjNfNtX77bEwXJodB8AnKd7nDcDGwKXA4Zl5Z3m/peU2c4GfAQdn5l3drV6dEhGPAS4A5gH3UwSq92TmTzv9HWEoQ5UkSZIkdcvQTf+TJEmSpG4yVEmSJElSBYYqSZIkSarAUCVJkiRJFRiqJEmSJKkCQ5Ukqa9ExIqIeH6PHvsxEXFxRKyJiH/uRQ1lHUdHxNm9enxJUns26nUBkiT1kTdSXOPqkZnpNUckSS1xpEqSNJQiYiZ/ONwGuKqbgWqGdUqS+ogX/5UkTSsiVgAnAq+jCB7fAQ7KzLURcTDwhszcu2b7DcB2mXltRJwB3A08HtgHuBJ4BfCPwEHArcD+mfmzmsc6BXgtsCVwPvDmzFxbrn8J8DFgW+Aq4LDM/HnNff8NOAAIYF5m/rnuuTwL+DSwPXA18I7MvLSs8wBgA3Av8NLM/F7N/R4P/AzYLDPXR8SpwNLM3KJcfxZweWZ+KiK2Ak4G9gZWAp/IzFPL7Y4GdgTWAn8DvAv4PnAGsCvwQyCBhZl5YEQ8HDgNeBEwF7gGeElm3tr8HZMkdZMjVZKkVr0KeCFFOPpfwMFt3vdI4FHAOuAy4Kfl7a8A/1K3/QHAXwNPpAg/RwJExC7A54E3AZtThK+vR8R4zX33B15MEUrqA9VmwLeAE8r7/wvwrYjYPDMPBs4BPpmZ82sDFUBm/g5YDexSLno2cGdEPKW8/RzgovL3LwE3AlsB+wEfj4i/qNnd0vJ5Lywf84vA5eXr8VGKsDnhIGBTYOuy5sOAe5Ak9Q2nHEiSWnVCZt4MEBHfAHZu475fy8zLy/t+DXhLZn6hvP1l4O/rtj8xM28o1x8DfIYiWL0ROCUzf1Rud2ZEHAHsyYOB5oSJ+zbwYuCazDyrvH1uRLwd2JdipGg6FwHPiYibyttfKW+vBR4JXBkRWwN7AS8uR9euiIjTKEb5flDe77LMPL98fo8Gdgeen5nrgIvL13fCfRRh6knliNzlLdQpSeoiQ5UkqVW31Px+N8UoTKtqp6rd0+D2/Lrta0PRdTWPtQ1wUES8rWb9w+pqaRaoKLe7rm7ZdcBjp7hPrYsopuzdCFwMXEgxTXEtcEk5LXArYGVmrql7jKc3qXErYFVm3lW3/dbl72eVv38pIhYCZwMfyMz7WqxZkjTLDFWSpKruAh4xcSMiFndgn1vX/L4EuLn8/QbgmMw8Zor7TnWy8M0UwazWEopzxFpxEfBPFKHqImAZxblTa3lwpOxmYLOIWFATrJYAN9Xsp7bG3wOLImJeTbBaMrFNGZ4+DHw4IrYFvk1xztXpLdYsSZplhipJUlVXAjtExM7Ar4GjO7DPt0bENylGxD4AfLlcfirwtYj4HvA/FGHuucDFdSNDzXwb+ExE/C3wHxQNM54KfLOVojLzmoi4BzgQODYzV0fEreV+Tiq3uSEiLgWOjYj3UJwTdijFeWKN9nldRPyEIjQdAexBMR3x6wAR8TyKNu9XUZzTdR+wvpV6JUndYaMKSVIlmXk18BHgexSd6ZZ1YLdfBP4v8FvgNxTd/sjMnwB/R9GJcBVwLW00zMjMO4CXAO8G7gDeS9FJ7/Y2arsIuKPmvK2LgDGKxhsT9qfoTngz8DXgqPrGF3X+FngGRafAo4Av1KxbTHHu1mrgV+XjnVW/A0lS79hSXZIkSZIqcKRKkiRJkiowVEmSJElSBYYqSZIkSarAUCVJkiRJFRiqJEmSJKkCQ5UkSZIkVWCokiRJkqQKDFWSJEmSVIGhSpIkSZIq+P+goCR+sdHgzgAAAABJRU5ErkJggg==\n","text/plain":["<Figure size 1008x360 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"lDpDxPurCdip"},"source":["df_ner_data_concat = pd.DataFrame({\"text\": texts_raw, \"tag_list\": labels_raw})\n","df_ner_data_concat.to_csv(DATA_CONCAT)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lBY-gqBPDCWy"},"source":["df = pd.read_csv(DATA_CONCAT)\n","labels = df[\"tag_list\"].astype(str).tolist()\n","texts = df[\"text\"].astype(str).tolist()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ef882zFB_akQ"},"source":["list_texts2idx = dict(zip(texts, range(len(texts))))\n","list_idx2text = dict(zip(range(len(texts)), texts))\n","list_labels2idx = dict(zip(labels, range(len(labels))))\n","list_idx2label = dict(zip(range(len(labels)), labels))\n","\n","texts = [list_texts2idx[text] for text in texts]\n","labels = [list_labels2idx[label] for label in labels]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vhNClpJqc4EF"},"source":["class CustomDataset(Dataset):\n","    def __init__(self, tokenizer, texts, labels, max_len, out_idx, pad_idx):\n","        self.len = len(texts)\n","        self.texts = texts\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","        self.out_idx = out_idx\n","        self.pad_idx = pad_idx\n","        \n","    def __getitem__(self, index):\n","        pad_token_idx = self.tokenizer.encode(\"[CLS]\")[0]\n","        sep_token_idx = self.tokenizer.encode(\"[SEP]\")[-1]\n","\n","        # Since Bert Tokenizer can create subwords, labels have to be extended accordingly\n","        ids_tmp, labels_tmp = [pad_token_idx], [self.out_idx]\n","        for word, label in zip(self.texts[index], self.labels[index]):\n","            word_tokenized = self.tokenizer.tokenize(word)\n","            token_ids = self.tokenizer.convert_tokens_to_ids(word_tokenized)\n","            ids_tmp.extend(token_ids)\n","            num_subwords = len(word_tokenized)\n","            labels_tmp.extend([label] * num_subwords)\n","\n","        ids_tmp.append(sep_token_idx)\n","        labels_tmp.append(self.out_idx)\n","\n","        ids_tmp.extend([0] * self.max_len) \n","        input_ids = ids_tmp[:self.max_len]\n","\n","        labels_tmp.extend([self.pad_idx] * self.max_len) \n","        tags = labels_tmp[:self.max_len]\n","\n","        input_masks = [int(i != 0) for i in input_ids]\n","        \n","        return {\n","            'ids': torch.tensor(input_ids, dtype=torch.long),\n","            'mask': torch.tensor(input_masks, dtype=torch.long),\n","            'tags': torch.tensor(tags, dtype=torch.long)\n","        } \n","    \n","    def __len__(self):\n","        return self.len"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nzU-L9Lsuye_"},"source":["# Training"]},{"cell_type":"code","metadata":{"id":"zPl0vw5zr8SC"},"source":["def get_true_and_pred_tags(y_true, y_preds, unique_labels):\n","    pred_tags = [unique_labels[p_i] for p, t in zip(y_preds, y_true)\n","                                  for p_i, t_i in zip(p, t) if unique_labels[t_i] != \"PAD\"]\n","    true_tags = [unique_labels[t_i] for t in y_true\n","                                  for t_i in t if unique_labels[t_i] != \"PAD\"]\n","\n","    return true_tags, pred_tags"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"crp70Dw8msy1"},"source":["def get_avg_metrics(y_true, y_preds, unique_labels):\n","    true_tags, pred_tags = get_true_and_pred_tags(y_true, y_preds, unique_labels)\n","    acc = accuracy_score(true_tags, pred_tags)\n","    f1 = f1_score(true_tags, pred_tags, average=\"macro\")\n","\n","    return acc, f1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H3Ba88Z8caZv"},"source":["def train_model(model, tokenizer, train_loader, valid_loader, epochs, patience, batch_size, unique_labels):\n","    # Total number of training steps is number of batches * number of epochs.\n","    total_steps = len(train_loader) * epochs\n","\n","    # Optimizer\n","    param_optimizer = list(model.named_parameters())\n","\n","    # apply weight decay to all parameters other than the following params\n","    # Experimenting with decaying certain components individually though -\n","    # requires for time for testing this optimization\n","    no_decay = ['bias', 'gamma', 'beta']\n","    \n","    optimizer_grouped_parameters = [\n","        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n","        'weight_decay_rate': 0.01},\n","        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n","        'weight_decay_rate': 0.0}\n","    ]\n","\n","    optimizer = AdamW(\n","        optimizer_grouped_parameters,\n","        lr=LEARNING_RATE,\n","        eps=1e-8\n","    )\n","\n","    # Create the learning rate scheduler.\n","    warmup_modifier = 0.1\n","    num_training_steps = int(len(train_loader) * epochs)\n","    scheduler = get_linear_schedule_with_warmup(\n","        optimizer,\n","        num_warmup_steps = num_training_steps * warmup_modifier, # Try with warmup steps\n","        num_training_steps = total_steps\n","    )\n","\n","    validate_every = len(train_loader) // 5\n","    print(f\"\\n===== Total Training Steps: {num_training_steps} = ({len(train_loader)} batches x {epochs} epochs) =====\")\n","\n","    model.train()\n","\n","    # Initialize complete tracking variables\n","    acc_train, f1_train, loss_train, acc_valid, f1_valid, loss_valid = [], [], [], [], [], []\n","    best_f1, early_stop, steps = 0, 0, 0\n","\n","    for epoch in trange(epochs, desc=\"Epoch\"):\n","        print(f\"\\n===== Epoch: {epoch + 1} =====\")\n","\n","        # Initialize individual tracking variables\n","        loss_tmp, loss_cnt = 0, 0\n","        y_pred_tmp, y_truth_tmp = [], []\n","\n","        for batch in train_loader:\n","            input_ids_batch = batch['ids'].to(device)\n","            input_mask_batch = batch['mask'].to(device)\n","            labels_batch = batch['tags'].to(device)\n","\n","            # Skip the last batch of which size is not equal to batch_size\n","            if labels_batch.size(0) != batch_size:\n","                break\n","\n","            steps += 1\n","\n","            # Reset gradient (default: gradients accumulate)\n","            model.zero_grad()\n","            optimizer.zero_grad()\n","\n","            # Initialise after the previous training\n","            if steps % validate_every == 1:\n","                y_pred_tmp, y_truth_tmp = [], []\n","\n","            # Perform a forward pass\n","            outputs = model(input_ids_batch, attention_mask=input_mask_batch, labels=labels_batch)\n","\n","            # Get the loss that comes with the model \n","            loss = outputs[0]\n","\n","            # Move logits and labels to CPU\n","            logits = outputs[1].detach().cpu().numpy()\n","            label_ids = labels_batch.to('cpu').numpy()\n","\n","            y_pred_tmp.extend([list(p) for p in np.argmax(logits, axis=2)])\n","            y_truth_tmp.extend(label_ids)\n","\n","            # Perform a backward pass to calculate the gradients.\n","            loss.backward()\n","\n","            # Track training loss\n","            loss_tmp += loss.item()\n","            loss_cnt += 1\n","\n","            # update weights and learning rate\n","            optimizer.step()\n","            scheduler.step()\n","\n","            #################### Evaluation ####################\n","            if (steps % validate_every == 0) or ((steps % validate_every != 0) and (steps == len(train_loader))):\n","\n","                # Evaluate Training\n","                acc, f1 = get_avg_metrics(y_truth_tmp, y_pred_tmp, unique_labels)\n","                acc_train.append(acc)\n","                f1_train.append(f1)\n","                loss_train.append(loss_tmp / loss_cnt)\n","                loss_tmp, loss_cnt = 0, 0\n","\n","                # Reset metrics\n","                y_truth_tmp, y_pred_tmp = [], []\n","\n","                # Evaluation mode\n","                model.eval()\n","\n","                # We do not need to update gradients during validation\n","                for batch in valid_loader:\n","                    input_ids_batch = batch['ids'].to(device)\n","                    input_mask_batch = batch['mask'].to(device)\n","                    labels_batch = batch['tags'].to(device)\n","\n","                    if labels_batch.size(0) != batch_size:\n","                      break\n","\n","                    with torch.no_grad():\n","                        outputs = model(input_ids_batch, attention_mask=input_mask_batch, labels=labels_batch)\n","\n","                    logits = outputs[1].detach().cpu().numpy()\n","                    label_ids = labels_batch.to('cpu').numpy()\n","                    y_pred_tmp.extend([list(p) for p in np.argmax(logits, axis=2)])\n","                    y_truth_tmp.extend(label_ids)\n","\n","                    # Validation Loss\n","                    loss = outputs[0]\n","                    loss_tmp += loss.item()\n","                    loss_cnt += 1\n","\n","                acc, f1 = get_avg_metrics(y_truth_tmp, y_pred_tmp, unique_labels)\n","                print(f\"Loss: {(loss_tmp / loss_cnt):.4f}, Acc: {acc:.4f}, F1: {f1:.4f}\")\n","\n","                acc_valid.append(acc)\n","                f1_valid.append(f1)\n","                loss_valid.append(loss_tmp / loss_cnt)\n","                loss_tmp, loss_cnt = 0, 0\n","\n","                # Prepare to train again for next epoch\n","                model.train()\n","\n","        #################### End of each epoch ####################\n","\n","        # Show the last evaluation metrics and classification report\n","        print(f\"\\nEpoch: {epoch+1}, Loss: {loss_valid[-1]:.4f}, Acc: {acc_valid[-1]:.4f}, F1: {f1_valid[-1]:.4f}, LR: {scheduler.get_last_lr()[0]:.2e}\\n\")\n","\n","        # Confusion Matrix\n","        true_tags, pred_tags = get_true_and_pred_tags(y_truth_tmp, y_pred_tmp, unique_labels)\n","        titles_options = [(\"Counts\", None), (\"Normalised\", 'true')]\n","        for title, normalize in titles_options:\n","            _, ax = plt.subplots(figsize=(15, 15))\n","            disp = skplt.metrics.plot_confusion_matrix(true_tags, pred_tags, normalize=normalize, title=title, x_tick_rotation=\"vertical\", ax=ax)\n","        plt.show()\n","\n","        # Plot training performance\n","        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,6))\n","        ax1.set_title(\"Losses\")\n","        ax1.set_xlabel(\"Validation Cycle\")\n","        ax1.set_ylabel(\"Loss\")\n","        ax1.plot(loss_train, \"b-o\", label=\"Train Loss\")\n","        ax1.plot(loss_valid, \"r-o\", label=\"Valid Loss\")\n","        ax1.legend(loc=\"upper right\")\n","        \n","        ax2.set_title(\"Evaluation\")\n","        ax2.set_xlabel(\"Validation Cycle\")\n","        ax2.set_ylabel(\"Score\")\n","        ax2.set_ylim(0,1)\n","        ax2.plot(acc_train, \"y-o\", label=\"Accuracy (train)\")\n","        ax2.plot(f1_train, \"y--\", label=\"F1 Score (train)\")\n","        ax2.plot(acc_valid, \"g-o\", label=\"Accuracy (valid)\")\n","        ax2.plot(f1_valid, \"g--\", label=\"F1 Score (valid)\")\n","        ax2.legend(loc=\"upper left\")\n","\n","        plt.show()\n","\n","        # If improving, save the number. If not, count up for early stopping\n","        if best_f1 < f1_valid[-1]:\n","            early_stop = 0\n","            best_f1 = f1_valid[-1]\n","            torch.save(model.state_dict(), MODEL_PATH)\n","        else:\n","            early_stop += 1\n","\n","        # Early stop if it reaches patience number\n","        if early_stop >= patience:\n","            break\n","\n","        # Prepare for the next epoch\n","        torch.cuda.empty_cache()\n","        model.train()\n","\n","    print(f\"############## cleaning up ###############\")\n","    del optimizer\n","    del scheduler\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","\n","    return loss_valid[-1], acc, f1, model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QEo-9HHe2ZIp"},"source":["def create_data_loaders_from_indices(X, y, train_indices, valid_indices, tokenizer, batch_size, max_len, pad_idx):\n","    print(f\"<<<< length train: {len(train_indices)}\")\n","    print(f\"<<<< length validation: {len(valid_indices)}\")\n","\n","    # Instead of TensorDataset try encode during model training\n","    X_train = np.array(X)[train_indices]\n","    y_train = np.array(y)[train_indices]\n","    train_data = CustomDataset(tokenizer, X_train, y_train, max_len, pad_idx)\n","\n","    X_valid = np.array(X)[valid_indices]\n","    y_valid = np.array(y)[valid_indices]\n","    validation_data = CustomDataset(tokenizer, X_valid, y_valid, max_len, pad_idx)\n","\n","    # DataLoaders instead of conventional for loops to avoid memory issue during training\n","    train_sampler = RandomSampler(train_data) # Randomize train data because good practice\n","    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","    validation_sampler = SequentialSampler(validation_data) # Validation does not need to be randomized\n","    valid_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n","\n","    return train_dataloader, valid_dataloader"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2BdvDpulT2jQ"},"source":["def train(n_splits, model_path, num_labels, unique_labels, df_text, df_labels, X, y, max_len, out_idx, pad_idx, epochs, patience, batch_size):\n","    print(\"\\n############### Start training ###############\")\n","    start_time = time.perf_counter() \n","    loss_folds, acc_folds, f1_folds = [], [], []\n","    model_final = None\n","\n","    kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n","    for idx, (train_indices, valid_indices) in enumerate(kfold.split(df_text, df_labels)):\n","        print(f\"\\n############### k-fold: {idx+1} ###############\")\n","\n","        # No lowercase for NER!\n","        tokenizer = DistilBertTokenizer.from_pretrained(model_path, do_lower_case=False) \n","        train_dataloader, valid_dataloader = create_data_loaders_from_indices(X, \n","                                                                              y,\n","                                                                              train_indices, \n","                                                                              valid_indices,\n","                                                                              tokenizer, \n","                                                                              batch_size,\n","                                                                              max_len,\n","                                                                              out_idx,\n","                                                                              pad_idx)\n","        \n","        # New model every fold\n","        model = DistilBertForTokenClassification.from_pretrained(model_path, num_labels=num_labels)\n","        model.to(device)\n","\n","        loss, acc, f1, model_trained = train_model(model,\n","                                              tokenizer,\n","                                              train_dataloader, \n","                                              valid_dataloader, \n","                                              epochs, \n","                                              patience, \n","                                              batch_size, \n","                                              unique_labels)\n","        loss_folds.append(loss)\n","        acc_folds.append(acc)\n","        f1_folds.append(f1)\n","\n","        # Save only the last model\n","        if idx == n_splits -1:\n","            model_final = model_trained\n","\n","    end_time = time.perf_counter()\n","    duration = end_time - start_time\n","    print(\"Process Time (sec): {}\".format(duration))\n","    print(f\"acc: {acc:.4f}, f1: {f1:.4f}, duration: {duration / 1000}\")\n","\n","    return loss_folds, acc_folds, f1_folds, model_final"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9CPLMBP99FX1"},"source":["# Using cased model here since NER works best this way\n","\n","NUM_FOLDS =  5#@param {type:\"number\"}\n","MODEL_PATH = 'distilbert-base-cased' #@param {type:\"string\"}\n","MAX_LEN =  200#@param {type:\"number\"}\n","BATCH_SIZE =  32#@param {type:\"number\"}\n","EPOCHS =  7#@param {type:\"number\"}\n","LEARNING_RATE = 2e-5 #@param {type:\"number\"}\n","PATIENCE = 3 #@param {type:\"number\"}\n","NUM_LABELS = num_labels\n","OUT_IDX = label2idx.get(\"O\")\n","PAD_IDX = label2idx.get(\"PAD\")\n","UNIQUE_LABELS = unique_labels_sorted"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ik9AD1kIK7FG","executionInfo":{"status":"ok","timestamp":1614726429721,"user_tz":-60,"elapsed":455,"user":{"displayName":"Carlos Tan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0VjHaxgzs8uvbhG9roTvit9LTXRto76Vdwkxh4w=s64","userId":"01125919987247037641"}},"outputId":"a31096a7-c4fd-4a18-c773-1f04bf5c6a3f"},"source":["# Example processed input data\n","tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased', do_lower_case=False)\n","tmp_data = CustomDataset(tokenizer, [texts_raw[5]], [labels_raw[5]], 50, OUT_IDX, PAD_IDX)\n","tmp_data[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'ids': tensor([  101,  8667,   117,   146,  2802,  6954,  1121,  1172,  1113,  1103,\n","           123,   119,   125,   119,  1627,   117,  1187,  1208,  1103,  2006,\n","          6753, 12063,  1228,  1170,   170,  1374,  1551,   119,   146,  1156,\n","          1176,  1106,  1862,  1122,   119,  1556,  1912, 12747,  7184, 20452,\n","          7272,  8741, 11588,   102,     0,     0,     0,     0,     0,     0]),\n"," 'mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n","         0, 0]),\n"," 'tags': tensor([8, 8, 8, 8, 8, 4, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n","         8, 8, 5, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 3, 8, 8, 8, 8, 8, 9, 9, 9, 9,\n","         9, 9])}"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1ibGcORS2hw_dFQwF1TOlu8w65F_alEaO"},"id":"M7dbkZQK2l2l","executionInfo":{"status":"ok","timestamp":1614456679203,"user_tz":-60,"elapsed":5222325,"user":{"displayName":"Carlos Tan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0VjHaxgzs8uvbhG9roTvit9LTXRto76Vdwkxh4w=s64","userId":"01125919987247037641"}},"outputId":"5f6d2bb7-f641-4dae-bd7e-dd353010f452"},"source":["%%time\n","loss_folds, acc_folds, f1_folds, model_final = train(NUM_FOLDS,\n","                                                     MODEL_PATH, \n","                                                     NUM_LABELS,\n","                                                     UNIQUE_LABELS,\n","                                                     texts,\n","                                                     labels,\n","                                                     texts_raw,\n","                                                     labels_raw,\n","                                                     MAX_LEN,\n","                                                     OUT_IDX,\n","                                                     PAD_IDX,\n","                                                     EPOCHS, \n","                                                     PATIENCE, \n","                                                     BATCH_SIZE)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"10vCJUYA2qKL","colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"status":"ok","timestamp":1614456679207,"user_tz":-60,"elapsed":5222148,"user":{"displayName":"Carlos Tan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0VjHaxgzs8uvbhG9roTvit9LTXRto76Vdwkxh4w=s64","userId":"01125919987247037641"}},"outputId":"583312f3-a99d-4880-fc71-1e4152ff0645"},"source":["results = pd.DataFrame({ \"loss\": loss_folds, \"acc\": acc_folds, \"f1\": f1_folds })\n","results"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>loss</th>\n","      <th>acc</th>\n","      <th>f1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.069928</td>\n","      <td>0.977216</td>\n","      <td>0.815072</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.068647</td>\n","      <td>0.979306</td>\n","      <td>0.787206</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.062063</td>\n","      <td>0.979450</td>\n","      <td>0.812019</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.065049</td>\n","      <td>0.977733</td>\n","      <td>0.800879</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.061575</td>\n","      <td>0.978973</td>\n","      <td>0.817457</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       loss       acc        f1\n","0  0.069928  0.977216  0.815072\n","1  0.068647  0.979306  0.787206\n","2  0.062063  0.979450  0.812019\n","3  0.065049  0.977733  0.800879\n","4  0.061575  0.978973  0.817457"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"code","metadata":{"id":"Wvr13xV62qHt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614456679208,"user_tz":-60,"elapsed":5219920,"user":{"displayName":"Carlos Tan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0VjHaxgzs8uvbhG9roTvit9LTXRto76Vdwkxh4w=s64","userId":"01125919987247037641"}},"outputId":"a79916b8-e426-4bd1-d968-d6ba815f365a"},"source":["results.to_csv(RESULTS_PATH)\n","results.mean()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["loss    0.065452\n","acc     0.978536\n","f1      0.806527\n","dtype: float64"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"markdown","metadata":{"id":"rTIJeOxeOduI"},"source":["# Infer"]},{"cell_type":"code","metadata":{"id":"ZxDz8MCtBuTu"},"source":["import nltk\n","from nltk.corpus import stopwords, wordnet\n","from nltk.stem import WordNetLemmatizer\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('wordnet')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"icvyV21KrjvZ"},"source":["SEQ_LEN = 200\n","# UNIQUE_LABELS = [\"BRND\", \"O\", \"PAD\", \"PRD\"]\n","UNIQUE_LABELS = ['B-BRND', 'B-MATR', 'B-MISC', 'B-PERS',\n","                 'B-PROD', 'B-TIME', 'I-BRND', 'I-PERS',\n","                 'O', 'PAD']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"66ooQYMiPUXg"},"source":["tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased', do_lower_case=False)\n","model = DistilBertForTokenClassification.from_pretrained(\"distilbert-base-cased\", num_labels=len(UNIQUE_LABELS))\n","model.load_state_dict(torch.load(MODEL_PATH, map_location=torch.device('cpu'))) # When CPU mode\n","#model.load_state_dict(torch.load(MODEL_PATH)) # for GPU mode\n","model.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eMXKKVoOG02u"},"source":["texts = [\"I have been dissatisfied for a long time, because I can not order more on invoice (why has always worked wonderfully for years) Now I have ordered the shoes, 38 way too small although I already have the shoe in black here and fits, and 39 too large. I suspect that the 38 declared wrong and actually a 37 or 36 is. If you keep the black shoe next to it, you will see a clear difference. Unfortunately, xxx does not mind me sending a new 38 and then using the existing, because they go back in any case. No, I would have to reorder and pay again in advance and the other will be refunded. I&#39;m honestly but too stupid now extra to go back to the bank, etc ... then I can go straight to the store. That was probably my last order here and I was a customer for many years and have bought a lot. A pity. Jane, D.\",\n","         \"My first delivery was not delivered, but I received goods from a buyer in the NL. Money for my order but was immediately booked back. Unfortunately, my order no longer received and shoes are no longer in my size there. Also no hats! :-(. Sincerely, John Doe\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"joGLa7hCO7rj"},"source":["def get_input_ids_and_tokens_list(tokenizer, texts):\n","    input_ids_list, tokens_list = [], []\n","    for text in texts:\n","        input_ids = tokenizer.encode(text)\n","        input_ids.extend([0] * SEQ_LEN)\n","        input_ids = input_ids[:SEQ_LEN]\n","        input_ids_list.append(input_ids)\n","        tokens = tokenizer.convert_ids_to_tokens(input_ids)\n","        tokens.extend([0] * SEQ_LEN)\n","        tokens = tokens[:SEQ_LEN]\n","        tokens_list.append(tokens)\n","\n","    return input_ids_list, tokens_list"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3XQbxwQDOwMW"},"source":["batch_size = 32\n","\n","input_ids_list, tokens_list = get_input_ids_and_tokens_list(tokenizer, texts)\n","predict_data = TensorDataset(torch.tensor(input_ids_list))\n","predict_sampler = SequentialSampler(predict_data) \n","predict_dataloader = DataLoader(predict_data, sampler=predict_sampler, batch_size=batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5zAUGiygQH1Z"},"source":["predicted_ids_list = []\n","model.eval()\n","for batch in predict_dataloader:\n","    with torch.no_grad():\n","        input_ids_batch = batch[0].to(device)\n","        output = model(input_ids_batch)\n","    predictions = np.argmax(output[0].to('cpu').numpy(), axis=2)\n","    predicted_ids_list.extend(predictions.tolist())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_3TgVQujROnU"},"source":["# Combine sub-words\n","tokens_cleaned_list, labels_cleaned_list = [], []\n","for tokens, label_ids in zip(tokens_list, predicted_ids_list):\n","    tokens_cleaned, labels_cleaned = [], []\n","    for token, id in zip(tokens, label_ids):\n","        if token.startswith(\"##\"):\n","            tokens_cleaned[-1] = tokens_cleaned[-1] + token[2:]\n","        else:\n","            labels_cleaned.append(UNIQUE_LABELS[id])\n","            tokens_cleaned.append(token)\n","\n","    tokens_cleaned_list.append(tokens_cleaned)\n","    labels_cleaned_list.append(labels_cleaned)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BwOz4uotFn7S"},"source":["labels_cleaned_list[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V6f2ZDLeScJD"},"source":["# Remove entries with no meaning and get unique entries\n","lemmatizer = WordNetLemmatizer()\n","\n","tokens_unique_list, labels_unique_list = [], []\n","for tokens, labels in zip(tokens_cleaned_list, labels_cleaned_list):\n","    tokens_filtered, labels_filtered = [], []\n","    for token, label in zip(tokens, labels):  \n","        if label != \"O\" and token != \"[PAD]\" and token != \"[SEP]\":\n","            print(\"{}\\t{}\".format(label, token))\n","            tokens_filtered.append(lemmatizer.lemmatize(token.lower(), \"n\"))\n","            labels_filtered.append(label)\n","\n","    tokens_unique_list.append(list(set(tokens_filtered)))\n","    labels_unique_list.append(list(set(labels_filtered)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7epr3MdLH0XO"},"source":["tokens_unique_list"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eSoTP7QuS3Zs"},"source":["labels_unique_list"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GiBAPiiKLTz3"},"source":["def get_entity_frequency(entities_list):\n","    entity_freq = dict()\n","    for entities in entities_list:\n","        for entity in entities:\n","            if entity in entity_freq:\n","                entity_freq[entity] += 1\n","            else:\n","                entity_freq[entity] = 1\n","\n","    return entity_freq"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z19mYHQUN0h1"},"source":["entity_freq = get_entity_frequency(tokens_unique_list)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"srTygvbVN7Oh"},"source":["entity_freq"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0nMdqA50N8fr"},"source":["df_entity_freq = pd.DataFrame({ \"entity\": entity_freq.keys(), \"entity_freq\": entity_freq.values()})\n","df_entity_freq.head()"],"execution_count":null,"outputs":[]}]}